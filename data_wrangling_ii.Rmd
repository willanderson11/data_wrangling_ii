---
title: "data_wrangling_ii"
author: "William Anderson"
date: "2022-10-13"
output: github_document
---

```{r}
library(tidyverse)
library(rvest)
library(httr)

```


## Extracting tables

This page contains data from the National Survey on Drug Use and Health; it includes tables for drug use in the past year or month, separately for specific kinds of drug use. These data are potentially useful for analysis, and we’d like to be able to read in the first table.

First, let’s make sure we can load the data from the web

```{r}
url = "http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm"
drug_use_html = read_html(url)

drug_use_html
```

Rather than trying to grab something using a CSS selector, let’s try our luck extracting the tables from the HTML.

```{r}
drug_use_html %>%
  html_table()
```

This has extracted all of the tables on the original page; that’s why we have a list with 15 elements. (We haven’t really talked about lists yet, but for now you can think of them as a general collection of objects in R. As we proceed, syntax for extracting individual elements from a list will become clear, and we’ll talk lots about lists in list columns.)

We’re only focused on the first table for now, so let’s get the contents from the first list element.

```{r}
table_marj = 
  drug_use_html %>%
  html_table() %>%
  first()
```

you’ll notice a problem: the “note” in the table appears in every column in the first row. We need to remove that…

```{r}
table_marj = 
  drug_use_html %>%
  html_table() %>%
  first() %>%
  slice(-1)

table_marj
```


## CSS Selectors

Suppose we’d like to scrape the data about the Star Wars Movies from the IMDB page. The first step is the same as before – we need to get the HTML.

```{r}
star_wars_html = 
  read_html("https://www.imdb.com/list/ls070150896/")
```

For each element, I’ll use the CSS selector in html_elements() to extract the relevant HTML code, and convert it to text. Then I can combine these into a data frame.

```{r}
title_vec = 
  star_wars_html %>%
  html_elements(".lister-item-header a") %>%
  html_text()

gross_rev_vec = 
  star_wars_html %>%
  html_elements(".text-small:nth-child(7) span:nth-child(5)") %>%
  html_text()

runtime_vec = 
  star_wars_html %>%
  html_elements(".runtime") %>%
  html_text()

star_wars_df = 
  tibble(
    title = title_vec,
    rev = gross_rev_vec,
    runtime = runtime_vec)

star_wars_df
```

## Using an API

New York City has a great open data resource, and we’ll use that for our API examples. Although most (all?) of these datasets can be accessed by clicking through a website, we’ll access them directly using the API to improve reproducibility and make it easier to update results to reflect new data.

As a simple example, this page is about a dataset for annual water consumption in NYC, along with the population in that year. First, we’ll import this as a CSV and parse it.

```{r}
nyc_water = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.csv") %>%
  content("parsed")
```

Data.gov also has a lot of data available using their API; often this is available as CSV or JSON as well. For example, we might be interested in data coming from BRFSS. This is importable via the API as a CSV (JSON, in this example, is more complicated).

```{r}
brfss_smart2010 = 
  GET("https://chronicdata.cdc.gov/resource/acme-vg9e.csv", query = list("$limit" = 5000)) %>%
  content("parsed")
```

By default, the CDC API limits data to the first 1000 rows. Here I’ve increased that by changing an element of the API query – I looked around the website describing the API to find the name of the argument, and then used the appropriate syntax for GET. To get the full data, I could increase this so that I get all the data at once or I could try iterating over chunks of a few thousand rows.

To get a sense of how this becomes complicated, let’s look at the Pokemon API (which is also pretty nice).

```{r}
poke = 
  GET("https://pokeapi.co/api/v2/pokemon/1") %>%
  content()

poke$name
poke$height
poke$abilities
```

To build a Pokemon dataset for analysis, you’d need to distill the data returned from the API into a useful format; iterate across all pokemon; and combine the results.

For both of the API examples we saw today, it wouldn’t be terrible to just download the CSV, document where it came from carefully, and move on. APIs are more helpful when the full dataset is complex and you only need pieces, or when the data are updated regularly
